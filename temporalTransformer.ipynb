{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eLHLKkqNjwdh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_device():\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p54tufrXj90t"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
        "        \"\"\"\n",
        "        # Add positional encoding to the input\n",
        "        x = x + self.pe[:x.size(1)].to(x.device)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "#Rotary positional encoding(RoPE)\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(RoPE, self).__init__()\n",
        "\n",
        "        #rotation matrix\n",
        "        self.rotation_matrix = torch.zeros((d_model, d_model), dtype=torch.devide(\"cuda\"))\n",
        "        for i in range(d_model):\n",
        "            for j in range(d_model):\n",
        "                self.rotation_matrix[i, j] = torch.cos(i*j*0.01)\n",
        "\n",
        "        #Positional encoding matrix\n",
        "        self.position_encoding = torch.zeros((max_len, d_model), dtype=torch.devide(\"cuda\"))\n",
        "        for i in range(max_len):\n",
        "            for j in range(d_model):\n",
        "                self.position_encoding[i, j] = torch.sin(i*j*0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: A tensor of shape (batch_size, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            A tensor of shape (batch_size, seq_len, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        #adding postional embedding to the input tensor\n",
        "        x = x + self.position_encoding\n",
        "\n",
        "        #aadding rotation matrix to the input tensor\n",
        "        x = torch.matmul(x, self.rotation_matrix)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape = parameters_shape\n",
        "        self.eps = eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out\n",
        "\n",
        "class VectorEmbedding(nn.Module):\n",
        "    #For a given vector embedding, process it for the transformer\n",
        "    def __init__(self, input_dim, d_model, max_sequence_length, drop_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, sequence_length, input_dim]\n",
        "        x = self.input_projection(x)\n",
        "        x = self.position_encoder(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FEATURE EXTRACTOR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FEATURE EXTRACTOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TemporalMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        \n",
        "        # For creating Q, K, V matrices\n",
        "        self.queries = nn.Linear(d_model, d_model)\n",
        "        self.keys = nn.Linear(d_model, d_model)\n",
        "        self.values = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        # W_u for concatenation of heads as shown in equation (11)\n",
        "        self.Wu = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key_value=None, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            query: Query tensor [batch_size, seq_len, d_model]\n",
        "            key_value: Key/Value tensor (if None, use query for self-attention)\n",
        "            mask: Optional mask for attention\n",
        "        \"\"\"\n",
        "        batch_size, query_len, _ = query.size()\n",
        "        \n",
        "        # For self-attention, key_value is the same as query\n",
        "        if key_value is None:\n",
        "            key_value = query\n",
        "        \n",
        "        key_len = key_value.size(1)\n",
        "        \n",
        "        # Create Q from query, K and V from key_value\n",
        "        Q = self.queries(query)\n",
        "        K = self.keys(key_value)\n",
        "        V = self.values(key_value)\n",
        "        \n",
        "        # Reshape for multi-head processing\n",
        "        Q = Q.reshape(batch_size, query_len, self.num_heads, self.head_dim)\n",
        "        K = K.reshape(batch_size, key_len, self.num_heads, self.head_dim)\n",
        "        V = V.reshape(batch_size, key_len, self.num_heads, self.head_dim)\n",
        "        \n",
        "        # Transpose for attention computation\n",
        "        Q = Q.permute(0, 2, 1, 3)  # [batch_size, num_heads, seq_len, head_dim]\n",
        "        K = K.permute(0, 2, 1, 3)\n",
        "        V = V.permute(0, 2, 1, 3)\n",
        "        \n",
        "        # Compute scaled dot-product attention\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "        attention_scores = attention_scores / math.sqrt(self.head_dim)\n",
        "        \n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "        # Apply softmax\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        \n",
        "        # Multiply by V\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        # Reshape back\n",
        "        context = context.permute(0, 2, 1, 3)  # [batch_size, seq_len, num_heads, head_dim]\n",
        "        context = context.reshape(batch_size, query_len, self.d_model)\n",
        "        \n",
        "        # Apply output projection\n",
        "        output = self.Wu(context)\n",
        "        \n",
        "        return output\n",
        "\n",
        "class SeparableConvolution(nn.Module):\n",
        "    def __init__(self, d_model, hidden, drop_prob):\n",
        "        super().__init__()\n",
        "        # Depthwise convolution (applies filters to each input channel separately)\n",
        "        self.depthwise_conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1, groups=d_model)\n",
        "        \n",
        "        # Pointwise convolutions (1x1 convolutions)\n",
        "        self.pointwise_conv1 = nn.Conv1d(d_model, hidden, kernel_size=1)\n",
        "        self.pointwise_conv2 = nn.Conv1d(hidden, d_model, kernel_size=1)\n",
        "        \n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, sequence_length, d_model]\n",
        "        # Transpose for Conv1d which expects [batch, channels, seq_len]\n",
        "        x = x.transpose(1, 2)\n",
        "        \n",
        "        # Apply depthwise convolution\n",
        "        x = self.depthwise_conv(x)\n",
        "        \n",
        "        # Apply first pointwise convolution with activation\n",
        "        x = self.pointwise_conv1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        # Apply second pointwise convolution\n",
        "        x = self.pointwise_conv2(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Transpose back to original format\n",
        "        x = x.transpose(1, 2)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ENCODER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ENCODER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LIdd1_z9kEOm"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super().__init__()\n",
        "        # First sub-layer: temporal self-attention\n",
        "        self.attention = TemporalMultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        \n",
        "        # Second sub-layer: separable convolution (replacing FFN)\n",
        "        self.separable_conv = SeparableConvolution(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, self_attention_mask=None):\n",
        "        # First sub-layer with residual connection and layer normalization\n",
        "        residual_x = x.clone()\n",
        "        x = self.attention(x, mask=self_attention_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + residual_x)\n",
        "        \n",
        "        # Second sub-layer with residual connection and layer normalization\n",
        "        residual_x = x.clone()\n",
        "        x = self.separable_conv(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 input_dim):\n",
        "        super().__init__()\n",
        "        self.vector_embedding = VectorEmbedding(input_dim, d_model, max_sequence_length, drop_prob)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob) \n",
        "                                   for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, self_attention_mask=None):\n",
        "        x = self.vector_embedding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, self_attention_mask)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DECODER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DECODER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2frmvZGIjrpW"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super().__init__()\n",
        "        # First sub-layer: masked temporal self-attention\n",
        "        self.masked_attention = TemporalMultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        \n",
        "        # Second sub-layer: multi-head attention over encoder output\n",
        "        self.encoder_decoder_attention = TemporalMultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "        \n",
        "        # Third sub-layer: separable convolution (replacing FFN)\n",
        "        self.separable_conv = SeparableConvolution(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, encoder_output, self_attention_mask=None, cross_attention_mask=None):\n",
        "        # First sub-layer with residual connection and layer normalization\n",
        "        residual = x.clone()\n",
        "        x = self.masked_attention(x, None, self_attention_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + residual)\n",
        "        \n",
        "        # Second sub-layer with residual connection and layer normalization\n",
        "        residual = x.clone()\n",
        "        x = self.encoder_decoder_attention(x, encoder_output, cross_attention_mask)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + residual)\n",
        "        \n",
        "        # Third sub-layer with residual connection and layer normalization\n",
        "        residual = x.clone()\n",
        "        x = self.separable_conv(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.norm3(x + residual)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 output_dim):\n",
        "        super().__init__()\n",
        "        self.vector_embedding = VectorEmbedding(output_dim, d_model, max_sequence_length, drop_prob)\n",
        "        self.rotary_positional_encoding = RoPE(d_model, max_sequence_length)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) \n",
        "                                   for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, encoder_output, self_attention_mask=None, cross_attention_mask=None):\n",
        "        # Apply vector embedding\n",
        "        x = self.vector_embedding(x)\n",
        "        \n",
        "        # Apply positional encoding as per equations (13) and (14)\n",
        "        x = self.rotary_positional_encoding(x)\n",
        "        \n",
        "        # Pass through decoder layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, self_attention_mask, cross_attention_mask)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TEMPORAL_TRANSFORMER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J34pPe_UkUBh"
      },
      "outputs": [],
      "source": [
        "class TemporalTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                input_dim,\n",
        "                output_dim,\n",
        "                d_model=32,\n",
        "                ffn_hidden=64,\n",
        "                num_heads=4,\n",
        "                drop_prob=0.1,\n",
        "                num_layers=3,\n",
        "                max_sequence_length=100):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            d_model=d_model,\n",
        "            ffn_hidden=ffn_hidden,\n",
        "            num_heads=num_heads,\n",
        "            drop_prob=drop_prob,\n",
        "            num_layers=num_layers,\n",
        "            max_sequence_length=max_sequence_length,\n",
        "            input_dim=input_dim\n",
        "        )\n",
        "        \n",
        "        self.decoder = Decoder(\n",
        "            d_model=d_model,\n",
        "            ffn_hidden=ffn_hidden,\n",
        "            num_heads=num_heads,\n",
        "            drop_prob=drop_prob,\n",
        "            num_layers=num_layers,\n",
        "            max_sequence_length=max_sequence_length,\n",
        "            output_dim=output_dim\n",
        "        )\n",
        "        \n",
        "        self.output_projection = nn.Linear(d_model, output_dim)\n",
        "        self.device = get_device()\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                tgt,\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: input sequence of shape [batch_size, seq_len, input_dim]\n",
        "            tgt: target sequence of shape [batch_size, seq_len, output_dim]\n",
        "        Returns:\n",
        "            output: predicted sequence of shape [batch_size, seq_len, output_dim]\n",
        "        \"\"\"\n",
        "        # Encode the input sequence\n",
        "        encoder_output = self.encoder(src, encoder_self_attention_mask)\n",
        "        \n",
        "        # Decode to get output sequence\n",
        "        decoder_output = self.decoder(\n",
        "            tgt,\n",
        "            encoder_output,\n",
        "            self_attention_mask=decoder_self_attention_mask,\n",
        "            cross_attention_mask=decoder_cross_attention_mask\n",
        "        )\n",
        "        \n",
        "        # Project to output dimension\n",
        "        output = self.output_projection(decoder_output)\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    # Add this method to the TemporalTransformer class\n",
        "    def get_decoder_mask(self, size):\n",
        "        \"\"\"Creates a causal mask for the decoder\"\"\"\n",
        "        # Lower triangular matrix\n",
        "        mask = torch.tril(torch.ones(size, size, device=self.device))\n",
        "        # Convert to boolean mask where 1=keep, 0=mask\n",
        "        return mask.unsqueeze(0).unsqueeze(1)  # Shape: [1, 1, size, size]\n",
        "\n",
        "    def predict(self, src, max_length):\n",
        "        #Generate output sequence given input sequence\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Encode the input sequence\n",
        "            encoder_output = self.encoder(src)\n",
        "            \n",
        "            # Initialize output sequence with zeros\n",
        "            batch_size = src.size(0)\n",
        "            output_dim = self.decoder.vector_embedding.input_projection.in_features\n",
        "            outputs = torch.zeros(batch_size, max_length, output_dim, device=src.device)\n",
        "            \n",
        "            # Generate output tokens one by one\n",
        "            for t in range(max_length):\n",
        "                # Get predictions up to current time step\n",
        "                current_output = outputs[:, :t+1, :]\n",
        "                \n",
        "                # Decode\n",
        "                decoder_output = self.decoder(\n",
        "                    current_output,\n",
        "                    encoder_output,\n",
        "                    self_attention_mask=self.get_decoder_mask(current_output.size(1)),\n",
        "                    cross_attention_mask=None\n",
        "                )\n",
        "                \n",
        "                # Get next prediction\n",
        "                next_output = self.output_projection(decoder_output[:, -1, :])\n",
        "                outputs[:, t, :] = next_output\n",
        "                \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'torch' has no attribute 'devide'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m tgt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, tgt_seq_len, output_dim)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTemporalTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mffn_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
            "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mTemporalTransformer.__init__\u001b[0;34m(self, input_dim, output_dim, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m Encoder(\n\u001b[1;32m     13\u001b[0m     d_model\u001b[38;5;241m=\u001b[39md_model,\n\u001b[1;32m     14\u001b[0m     ffn_hidden\u001b[38;5;241m=\u001b[39mffn_hidden,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39minput_dim\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mDecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mffn_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mffn_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dim\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_projection \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(d_model, output_dim)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m get_device()\n",
            "Cell \u001b[0;32mIn[5], line 51\u001b[0m, in \u001b[0;36mDecoder.__init__\u001b[0;34m(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, output_dim)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_embedding \u001b[38;5;241m=\u001b[39m VectorEmbedding(output_dim, d_model, max_sequence_length, drop_prob)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_positional_encoding \u001b[38;5;241m=\u001b[39m \u001b[43mRoPE\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) \n\u001b[1;32m     53\u001b[0m                            \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)])\n",
            "Cell \u001b[0;32mIn[2], line 29\u001b[0m, in \u001b[0;36mRoPE.__init__\u001b[0;34m(self, d_model, max_len)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28msuper\u001b[39m(RoPE, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#rotation matrix\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((d_model, d_model), dtype\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevide\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d_model):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d_model):\n",
            "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/torch/__init__.py:2681\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 2681\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'devide'"
          ]
        }
      ],
      "source": [
        "# Define test parameters\n",
        "batch_size = 2\n",
        "src_seq_len = 10  # source sequence length\n",
        "tgt_seq_len = 8   # target sequence length\n",
        "input_dim = 16    # dimensionality of input features\n",
        "output_dim = 4    # dimensionality of output features\n",
        "\n",
        "# Create dummy input data\n",
        "src = torch.randn(batch_size, src_seq_len, input_dim)\n",
        "tgt = torch.randn(batch_size, tgt_seq_len, output_dim)\n",
        "\n",
        "# Initialize the model\n",
        "model = TemporalTransformer(\n",
        "    input_dim=input_dim,\n",
        "    output_dim=output_dim,\n",
        "    d_model=32,\n",
        "    ffn_hidden=64,\n",
        "    num_heads=4,\n",
        "    drop_prob=0.1,\n",
        "    num_layers=2,\n",
        "    max_sequence_length=100\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "src = src.to(device)\n",
        "tgt = tgt.to(device)\n",
        "\n",
        "# Forward pass\n",
        "try:\n",
        "    output = model(src, tgt)\n",
        "    print(f\"Forward pass successful!\")\n",
        "    print(f\"Input shape: {src.shape}\")\n",
        "    print(f\"Target shape: {tgt.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Forward pass error: {str(e)}\")\n",
        "\n",
        "# Test prediction functionality\n",
        "try:\n",
        "    predicted = model.predict(src, max_length=tgt_seq_len)\n",
        "    print(f\"Prediction successful!\")\n",
        "    print(f\"Predicted shape: {predicted.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Prediction error: {str(e)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
