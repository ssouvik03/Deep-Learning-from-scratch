{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# THIS IS NOT THE FINAL VERSION(NON WORKING VERSION AS  OF NOW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eLHLKkqNjwdh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_device():\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p54tufrXj90t"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
        "        \"\"\"\n",
        "        # Add positional encoding to the input\n",
        "        x = x + self.pe[:x.size(1)]\n",
        "        return x\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape = parameters_shape\n",
        "        self.eps = eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out\n",
        "\n",
        "class VectorEmbedding(nn.Module):\n",
        "    #For a given vector embedding, process it for the transformer\n",
        "    def __init__(self, input_dim, d_model, max_sequence_length, drop_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, sequence_length, input_dim]\n",
        "        x = self.input_projection(x)\n",
        "        x = self.position_encoder(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FEATURE EXTRACTOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TemporalMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        \n",
        "        # For creating Q, K, V matrices\n",
        "        self.queries = nn.Linear(d_model, d_model)\n",
        "        self.keys = nn.Linear(d_model, d_model)\n",
        "        self.values = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        # W_u for concatenation of heads as shown in equation (11)\n",
        "        self.Wu = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, sequence_length, d_model = x.size()\n",
        "        \n",
        "        # Create Q, K, V matrices for input nodes\n",
        "        Q = self.queries(x)\n",
        "        K = self.keys(x)\n",
        "        V = self.values(x)\n",
        "        \n",
        "        # Reshape for multi-head processing\n",
        "        Q = Q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
        "        K = K.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
        "        V = V.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
        "        \n",
        "        # Transpose for attention computation\n",
        "        Q = Q.permute(0, 2, 1, 3)  # [batch_size, num_heads, seq_len, head_dim]\n",
        "        K = K.permute(0, 2, 1, 3)\n",
        "        V = V.permute(0, 2, 1, 3)\n",
        "        \n",
        "        # Compute scaled dot-product as in equation (12)\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "        attention_scores = attention_scores / math.sqrt(self.head_dim)\n",
        "        \n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "        # Apply softmax to get head_i\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        \n",
        "        # Multiply by V to complete head_i calculation\n",
        "        context = torch.matmul(attention_weights, V)  # [batch_size, num_heads, seq_len, head_dim]\n",
        "        \n",
        "        # Reshape back and concatenate heads\n",
        "        context = context.permute(0, 2, 1, 3)  # [batch_size, seq_len, num_heads, head_dim]\n",
        "        context = context.reshape(batch_size, sequence_length, self.d_model)\n",
        "        \n",
        "        # Apply W_u to complete equation (11): MultiHead_i = W_u Â· concat([head_i0, ..., head_ih])\n",
        "        output = self.Wu(context)\n",
        "        \n",
        "        return output\n",
        "\n",
        "class SeparableConvolution(nn.Module):\n",
        "    def __init__(self, d_model, hidden, drop_prob):\n",
        "        super().__init__()\n",
        "        # Depthwise convolution (applies filters to each input channel separately)\n",
        "        self.depthwise_conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1, groups=d_model)\n",
        "        \n",
        "        # Pointwise convolutions (1x1 convolutions)\n",
        "        self.pointwise_conv1 = nn.Conv1d(d_model, hidden, kernel_size=1)\n",
        "        self.pointwise_conv2 = nn.Conv1d(hidden, d_model, kernel_size=1)\n",
        "        \n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, sequence_length, d_model]\n",
        "        # Transpose for Conv1d which expects [batch, channels, seq_len]\n",
        "        x = x.transpose(1, 2)\n",
        "        \n",
        "        # Apply depthwise convolution\n",
        "        x = self.depthwise_conv(x)\n",
        "        \n",
        "        # Apply first pointwise convolution with activation\n",
        "        x = self.pointwise_conv1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        # Apply second pointwise convolution\n",
        "        x = self.pointwise_conv2(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Transpose back to original format\n",
        "        x = x.transpose(1, 2)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ENCODER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LIdd1_z9kEOm"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super().__init__()\n",
        "        # First sub-layer: temporal self-attention\n",
        "        self.attention = TemporalMultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        \n",
        "        # Second sub-layer: separable convolution (replacing FFN)\n",
        "        self.separable_conv = SeparableConvolution(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, self_attention_mask=None):\n",
        "        # First sub-layer with residual connection and layer normalization\n",
        "        residual_x = x.clone()\n",
        "        x = self.attention(x, mask=self_attention_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + residual_x)\n",
        "        \n",
        "        # Second sub-layer with residual connection and layer normalization\n",
        "        residual_x = x.clone()\n",
        "        x = self.separable_conv(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 input_dim):\n",
        "        super().__init__()\n",
        "        self.vector_embedding = VectorEmbedding(input_dim, d_model, max_sequence_length, drop_prob)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob) \n",
        "                                   for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, self_attention_mask=None):\n",
        "        x = self.vector_embedding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, self_attention_mask)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DECODER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2frmvZGIjrpW"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super().__init__()\n",
        "        # First sub-layer: masked temporal self-attention\n",
        "        self.masked_attention = TemporalMultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        \n",
        "        # Second sub-layer: multi-head attention over encoder output\n",
        "        self.encoder_decoder_attention = TemporalMultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "        \n",
        "        # Third sub-layer: separable convolution (replacing FFN)\n",
        "        self.separable_conv = SeparableConvolution(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, encoder_output, self_attention_mask=None, cross_attention_mask=None):\n",
        "        # First sub-layer with residual connection and layer normalization\n",
        "        residual = x.clone()\n",
        "        x = self.masked_attention(x, mask=self_attention_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + residual)\n",
        "        \n",
        "        # Second sub-layer with residual connection and layer normalization\n",
        "        residual = x.clone()\n",
        "        x = self.encoder_decoder_attention(\n",
        "            x, \n",
        "            encoder_output, \n",
        "            mask=cross_attention_mask\n",
        "        )\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + residual)\n",
        "        \n",
        "        # Third sub-layer with residual connection and layer normalization\n",
        "        residual = x.clone()\n",
        "        x = self.separable_conv(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.norm3(x + residual)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 output_dim):\n",
        "        super().__init__()\n",
        "        self.vector_embedding = VectorEmbedding(output_dim, d_model, max_sequence_length, drop_prob)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) \n",
        "                                   for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, encoder_output, self_attention_mask=None, cross_attention_mask=None):\n",
        "        # Apply vector embedding\n",
        "        x = self.vector_embedding(x)\n",
        "        \n",
        "        # Apply positional encoding as per equations (13) and (14)\n",
        "        x = self.positional_encoding(x)\n",
        "        \n",
        "        # Pass through decoder layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, self_attention_mask, cross_attention_mask)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TEMPORAL_TRANSFORMER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J34pPe_UkUBh"
      },
      "outputs": [],
      "source": [
        "class TemporalTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                input_dim,\n",
        "                output_dim,\n",
        "                d_model=32,\n",
        "                ffn_hidden=64,\n",
        "                num_heads=4,\n",
        "                drop_prob=0.1,\n",
        "                num_layers=3,\n",
        "                max_sequence_length=100):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            d_model=d_model,\n",
        "            ffn_hidden=ffn_hidden,\n",
        "            num_heads=num_heads,\n",
        "            drop_prob=drop_prob,\n",
        "            num_layers=num_layers,\n",
        "            max_sequence_length=max_sequence_length,\n",
        "            input_dim=input_dim\n",
        "        )\n",
        "        \n",
        "        self.decoder = Decoder(\n",
        "            d_model=d_model,\n",
        "            ffn_hidden=ffn_hidden,\n",
        "            num_heads=num_heads,\n",
        "            drop_prob=drop_prob,\n",
        "            num_layers=num_layers,\n",
        "            max_sequence_length=max_sequence_length,\n",
        "            output_dim=output_dim\n",
        "        )\n",
        "        \n",
        "        self.output_projection = nn.Linear(d_model, output_dim)\n",
        "        self.device = get_device()\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                tgt,\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: input sequence of shape [batch_size, seq_len, input_dim]\n",
        "            tgt: target sequence of shape [batch_size, seq_len, output_dim]\n",
        "        Returns:\n",
        "            output: predicted sequence of shape [batch_size, seq_len, output_dim]\n",
        "        \"\"\"\n",
        "        # Encode the input sequence\n",
        "        encoder_output = self.encoder(src, encoder_self_attention_mask)\n",
        "        \n",
        "        # Decode to get output sequence\n",
        "        decoder_output = self.decoder(\n",
        "            tgt,\n",
        "            encoder_output,\n",
        "            self_attention_mask=decoder_self_attention_mask,\n",
        "            cross_attention_mask=decoder_cross_attention_mask\n",
        "        )\n",
        "        \n",
        "        # Project to output dimension\n",
        "        output = self.output_projection(decoder_output)\n",
        "        \n",
        "        return output\n",
        "\n",
        "    def predict(self, src, max_length):\n",
        "        #Generate output sequence given input sequence\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Encode the input sequence\n",
        "            encoder_output = self.encoder(src)\n",
        "            \n",
        "            # Initialize output sequence with zeros\n",
        "            batch_size = src.size(0)\n",
        "            output_dim = self.decoder.vector_embedding.input_projection.in_features\n",
        "            outputs = torch.zeros(batch_size, max_length, output_dim).to(self.device)\n",
        "            \n",
        "            # Generate output tokens one by one\n",
        "            for t in range(max_length):\n",
        "                # Get predictions up to current time step\n",
        "                current_output = outputs[:, :t+1, :]\n",
        "                \n",
        "                # Decode\n",
        "                decoder_output = self.decoder(\n",
        "                    current_output,\n",
        "                    encoder_output,\n",
        "                    self_attention_mask=self.get_decoder_mask(current_output.size(1))\n",
        "                )\n",
        "                \n",
        "                # Get next prediction\n",
        "                next_output = self.output_projection(decoder_output[:, -1, :])\n",
        "                outputs[:, t, :] = next_output\n",
        "                \n",
        "        return outputs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
